{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3DR-eO17geWu"},"source":["# Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EMefrVPCg-60"},"source":["### Importing the libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"sCV30xyVhFbE"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"FIleuCAjoFD8"},"outputs":[{"data":{"text/plain":["'2.17.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oxQxCBWyoGPE"},"source":["## Part 1 - Data Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing the Training set\n","\n","\n","> Applying preprocessing only to the training set to prevent overfitting and apply data variations through geometric transformations(zoom,rotation,flipping etc.) of image, also called \"**Image Augmentation**\".\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"0koUcJMJpEBD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8000 images belonging to 2 classes.\n"]}],"source":["train_datagen = ImageDataGenerator(rescale = 1./255, # Apply feature scalling to each pixel by dividing the value by 255\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","training_set = train_datagen.flow_from_directory('dataset/training_set',\n","                                                 target_size = (64, 64), # Size of the input images; reduced to increase processing speed.\n","                                                 batch_size = 32,\n","                                                 class_mode = 'binary')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mrCMmGw9pHys"},"source":["### Preprocessing the Test set"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"SH4WzfOhpKc3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n"]}],"source":["test_datagen = ImageDataGenerator(rescale = 1./255) # Only Apply feature scalling to each pixel\n","test_set = test_datagen.flow_from_directory('dataset/test_set',\n","                                            target_size = (64, 64),\n","                                            batch_size = 32,\n","                                            class_mode = 'binary')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"af8O4l90gk7B"},"source":["## Part 2 - Building the CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ces1gXY2lmoX"},"source":["### Initialising the CNN"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"SAUt4UMPlhLS"},"outputs":[],"source":["cnn = tf.keras.models.Sequential()"]},{"cell_type":"markdown","metadata":{},"source":["### Step 1 - Convolution\n","#### Conv2D Layer Parameters:\n","\n","\n","> **filter**-> how many feature detector we want, <br>\n","  **kernel_size** -> easch feature detector matrix of 3x3 size ,<br>\n","  **activation** -> activation function used for convolutional layer ( Rectifier function )<br>\n","  **input_shape** -> input image er size, and '3' for colored image; \"1\" if black and white image  \n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"XPzPrMckl-hV"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tf87FpvxmNOJ"},"source":["### Step 2 - Pooling"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{},"colab_type":"code","id":"ncpqPl69mOac"},"outputs":[],"source":["cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xaTOgD8rm4mU"},"source":["### Adding a second convolutional layer"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{},"colab_type":"code","id":"i_-FZjn_m8gk"},"outputs":[],"source":["cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n","cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tmiEuvTunKfk"},"source":["### Step 3 - Flattening"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{},"colab_type":"code","id":"6AZeOGCvnNZn"},"outputs":[],"source":["cnn.add(tf.keras.layers.Flatten())"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dAoSECOm203v"},"source":["### Step 4 - Full Connection"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{},"colab_type":"code","id":"8GtmUlLd26Nq"},"outputs":[],"source":["cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yTldFvbX28Na"},"source":["### Step 5 - Output Layer"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{},"colab_type":"code","id":"1p_Zj1Mc3Ko_"},"outputs":[],"source":["cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D6XkI90snSDl"},"source":["## Part 3 - Training the CNN"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vfrFQACEnc6i"},"source":["### Compiling the CNN"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{},"colab_type":"code","id":"NALksrNQpUlJ"},"outputs":[],"source":["cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ehS-v3MIpX2h"},"source":["### Training the CNN on the Training set and evaluating it on the Test set"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{},"colab_type":"code","id":"XUj1W4PJptta"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/25\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.5646 - loss: 0.6769 - val_accuracy: 0.7020 - val_loss: 0.5995\n","Epoch 2/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 357ms/step - accuracy: 0.6826 - loss: 0.6067 - val_accuracy: 0.7275 - val_loss: 0.5692\n","Epoch 3/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.7198 - loss: 0.5565 - val_accuracy: 0.7300 - val_loss: 0.5425\n","Epoch 4/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7275 - loss: 0.5290 - val_accuracy: 0.7595 - val_loss: 0.5041\n","Epoch 5/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.7475 - loss: 0.5069 - val_accuracy: 0.7760 - val_loss: 0.4876\n","Epoch 6/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7672 - loss: 0.4779 - val_accuracy: 0.7725 - val_loss: 0.4834\n","Epoch 7/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7748 - loss: 0.4717 - val_accuracy: 0.7445 - val_loss: 0.5238\n","Epoch 8/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7763 - loss: 0.4556 - val_accuracy: 0.7860 - val_loss: 0.4637\n","Epoch 9/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.7992 - loss: 0.4247 - val_accuracy: 0.7935 - val_loss: 0.4582\n","Epoch 10/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 229ms/step - accuracy: 0.7858 - loss: 0.4400 - val_accuracy: 0.7565 - val_loss: 0.5037\n","Epoch 11/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 233ms/step - accuracy: 0.8151 - loss: 0.4071 - val_accuracy: 0.7985 - val_loss: 0.4518\n","Epoch 12/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.8078 - loss: 0.4083 - val_accuracy: 0.7885 - val_loss: 0.4666\n","Epoch 13/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8149 - loss: 0.4032 - val_accuracy: 0.7870 - val_loss: 0.4833\n","Epoch 14/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8304 - loss: 0.3799 - val_accuracy: 0.7980 - val_loss: 0.4594\n","Epoch 15/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8382 - loss: 0.3706 - val_accuracy: 0.7945 - val_loss: 0.4757\n","Epoch 16/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.8467 - loss: 0.3476 - val_accuracy: 0.8005 - val_loss: 0.4573\n","Epoch 17/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 240ms/step - accuracy: 0.8471 - loss: 0.3414 - val_accuracy: 0.8035 - val_loss: 0.4678\n","Epoch 18/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 237ms/step - accuracy: 0.8601 - loss: 0.3128 - val_accuracy: 0.7960 - val_loss: 0.4611\n","Epoch 19/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 236ms/step - accuracy: 0.8556 - loss: 0.3137 - val_accuracy: 0.7875 - val_loss: 0.5362\n","Epoch 20/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.8657 - loss: 0.3066 - val_accuracy: 0.8010 - val_loss: 0.4578\n","Epoch 21/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 234ms/step - accuracy: 0.8818 - loss: 0.2751 - val_accuracy: 0.8035 - val_loss: 0.4644\n","Epoch 22/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 229ms/step - accuracy: 0.8957 - loss: 0.2571 - val_accuracy: 0.8120 - val_loss: 0.4685\n","Epoch 23/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 240ms/step - accuracy: 0.8797 - loss: 0.2725 - val_accuracy: 0.7790 - val_loss: 0.5568\n","Epoch 24/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 239ms/step - accuracy: 0.8948 - loss: 0.2518 - val_accuracy: 0.7845 - val_loss: 0.5193\n","Epoch 25/25\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 235ms/step - accuracy: 0.9081 - loss: 0.2317 - val_accuracy: 0.8075 - val_loss: 0.5106\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x17cfdf99b20>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"U3PZasO0006Z"},"source":["## Part 4 - Making a single prediction"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{},"colab_type":"code","id":"gsSiWEJY1BPB"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"]},{"data":{"text/plain":["{'cats': 0, 'dogs': 1}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from tensorflow.keras.preprocessing import image\n","test_image = image.load_img('dataset/single_prediction/cat_or_dog_5.jpeg', target_size = (64, 64)) # cat\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = cnn.predict(test_image)\n","training_set.class_indices"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{},"colab_type":"code","id":"ED9KB3I54c1i"},"outputs":[{"name":"stdout","output_type":"stream","text":["cat\n"]}],"source":["print( 'dog' if result[0][0] == 1 else 'cat' )"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n","dog\n"]}],"source":["test_image = image.load_img('dataset/single_prediction/cat_or_dog_7.jpeg', target_size = (64, 64)) # dog\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = cnn.predict(test_image)\n","print( 'dog' if result[0][0] == 1 else 'cat' )\n"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n","cat\n"]}],"source":["test_image = image.load_img('dataset/single_prediction/cat_or_dog_4.jpg', target_size = (64, 64)) # cat\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = cnn.predict(test_image)\n","print( 'dog' if result[0][0] == 1 else 'cat' )\n"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n","dog\n"]}],"source":["test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64)) # dog\n","test_image = image.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = cnn.predict(test_image)\n","print( 'dog' if result[0][0] == 1 else 'cat' )\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyON0YxX/oky4tPbqCLnFjWD","collapsed_sections":[],"name":"convolutional_neural_network.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
