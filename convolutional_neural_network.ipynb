{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DR-eO17geWu"
      },
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "[Building powerful image classification models using very little data - Keras Blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMefrVPCg-60"
      },
      "source": [
        "### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sCV30xyVhFbE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIleuCAjoFD8",
        "outputId": "c7a3ce0a-2376-4041-bf3e-fafbae53c495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.17.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxQxCBWyoGPE"
      },
      "source": [
        "## Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4kZf8whYO-Y"
      },
      "source": [
        "### Preprocessing the Training set\n",
        "\n",
        "\n",
        "> Applying preprocessing only to the training set to prevent overfitting and apply data variations through geometric transformations(zoom,rotation,flipping etc.) of image, also called \"**Image Augmentation**\".\n",
        "\n",
        "[Data Augmentation Code Snippet Copied from Keras Blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
        "\n",
        "> <u>**rescale:**</u> a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our models to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255 factor. <br>\n",
        "<u>**shear_range**:</u> randomly applying shearing transformations (transformation in which all points along a given line remain fixed while other points are shifted parallel to by a distance proportional to their perpendicular distance from. Shearing a plane figure does not change its area or stretch dimensions, only changes interior angles. <br>\n",
        "<u>**zoom_range**:</u> randomly zooming inside pictures <br>\n",
        "horizontal_flip: randomly flipping half of the images horizontally --relevant when there are no assumptions of horizontal assymetry (e.g. real-world pictures). <br>\n",
        "<u>**Example for Horizontal Flip:**</u><br>\n",
        "\"F\" -> Mirror image of \"F\"<br>\n",
        "\"}\" -> \"{\"<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0koUcJMJpEBD",
        "outputId": "8838ac1c-b2e9-420c-b1a3-bf860cfaef8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255, # Apply feature scalling to each pixel by dividing the value by 255\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
        "                                                 target_size = (64, 64), # Size of the input images; reduced to increase processing speed.\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCMmGw9pHys"
      },
      "source": [
        "### Preprocessing the Test set\n",
        "\n",
        "> <u>**flow_from_directory():**</u> this is a generator that will read pictures found in subfolders of 'dataset/training_set', and indefinitely generate batches of augmented image data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SH4WzfOhpKc3",
        "outputId": "bed888a0-660f-4146-85cc-b8d07985f488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255) # Only Apply feature scalling to each pixel\n",
        "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
        "                                            target_size = (64, 64), # all images will be resized to 64x64\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary') # since we use binary_crossentropy loss, we need binary labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af8O4l90gk7B"
      },
      "source": [
        "## Part 2 - Building the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ces1gXY2lmoX"
      },
      "source": [
        "### Initialising the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SAUt4UMPlhLS"
      },
      "outputs": [],
      "source": [
        "cnn = tf.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky-ktPmzYO-j"
      },
      "source": [
        "### Step 1 - Convolution\n",
        "#### Conv2D Layer Parameters:\n",
        "\n",
        "\n",
        "> **filter**-> how many feature detector we want, <br>\n",
        "  **kernel_size** -> easch feature detector matrix of 3x3 size ,<br>\n",
        "  **activation** -> activation function used for convolutional layer ( Rectifier function )<br>\n",
        "  **input_shape** -> input image er size, and '3' for colored image; \"1\" if black and white image  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPzPrMckl-hV",
        "outputId": "c6b39d2f-5770-4e4c-ecf7-96409251a035"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf87FpvxmNOJ"
      },
      "source": [
        "### Step 2 - Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncpqPl69mOac"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaTOgD8rm4mU"
      },
      "source": [
        "### Adding a second convolutional layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_-FZjn_m8gk"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "# the model outputs 3D feature maps (height, width, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmiEuvTunKfk"
      },
      "source": [
        "### Step 3 - Flattening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AZeOGCvnNZn"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Flatten())  # this converts our 3D feature maps to 1D feature vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAoSECOm203v"
      },
      "source": [
        "### Step 4 - Full Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GtmUlLd26Nq"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTldFvbX28Na"
      },
      "source": [
        "### Step 5 - Output Layer\n",
        "\n",
        "> We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p_Zj1Mc3Ko_"
      },
      "outputs": [],
      "source": [
        "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XkI90snSDl"
      },
      "source": [
        "## Part 3 - Training the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrFQACEnc6i"
      },
      "source": [
        "### Compiling the CNN\n",
        "\n",
        "> To go with the output layer, we will also use the \"**binary_crossentropy**\" loss to train our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NALksrNQpUlJ"
      },
      "outputs": [],
      "source": [
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehS-v3MIpX2h"
      },
      "source": [
        "### Training the CNN on the Training set and evaluating it on the Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUj1W4PJptta",
        "outputId": "cc526605-bbb9-4551-ab4a-76fa306e815c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 1s/step - accuracy: 0.5646 - loss: 0.6769 - val_accuracy: 0.7020 - val_loss: 0.5995\n",
            "Epoch 2/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 357ms/step - accuracy: 0.6826 - loss: 0.6067 - val_accuracy: 0.7275 - val_loss: 0.5692\n",
            "Epoch 3/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.7198 - loss: 0.5565 - val_accuracy: 0.7300 - val_loss: 0.5425\n",
            "Epoch 4/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7275 - loss: 0.5290 - val_accuracy: 0.7595 - val_loss: 0.5041\n",
            "Epoch 5/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.7475 - loss: 0.5069 - val_accuracy: 0.7760 - val_loss: 0.4876\n",
            "Epoch 6/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7672 - loss: 0.4779 - val_accuracy: 0.7725 - val_loss: 0.4834\n",
            "Epoch 7/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7748 - loss: 0.4717 - val_accuracy: 0.7445 - val_loss: 0.5238\n",
            "Epoch 8/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.7763 - loss: 0.4556 - val_accuracy: 0.7860 - val_loss: 0.4637\n",
            "Epoch 9/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.7992 - loss: 0.4247 - val_accuracy: 0.7935 - val_loss: 0.4582\n",
            "Epoch 10/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 229ms/step - accuracy: 0.7858 - loss: 0.4400 - val_accuracy: 0.7565 - val_loss: 0.5037\n",
            "Epoch 11/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 233ms/step - accuracy: 0.8151 - loss: 0.4071 - val_accuracy: 0.7985 - val_loss: 0.4518\n",
            "Epoch 12/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 231ms/step - accuracy: 0.8078 - loss: 0.4083 - val_accuracy: 0.7885 - val_loss: 0.4666\n",
            "Epoch 13/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8149 - loss: 0.4032 - val_accuracy: 0.7870 - val_loss: 0.4833\n",
            "Epoch 14/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8304 - loss: 0.3799 - val_accuracy: 0.7980 - val_loss: 0.4594\n",
            "Epoch 15/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 230ms/step - accuracy: 0.8382 - loss: 0.3706 - val_accuracy: 0.7945 - val_loss: 0.4757\n",
            "Epoch 16/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.8467 - loss: 0.3476 - val_accuracy: 0.8005 - val_loss: 0.4573\n",
            "Epoch 17/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 240ms/step - accuracy: 0.8471 - loss: 0.3414 - val_accuracy: 0.8035 - val_loss: 0.4678\n",
            "Epoch 18/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 237ms/step - accuracy: 0.8601 - loss: 0.3128 - val_accuracy: 0.7960 - val_loss: 0.4611\n",
            "Epoch 19/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 236ms/step - accuracy: 0.8556 - loss: 0.3137 - val_accuracy: 0.7875 - val_loss: 0.5362\n",
            "Epoch 20/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 232ms/step - accuracy: 0.8657 - loss: 0.3066 - val_accuracy: 0.8010 - val_loss: 0.4578\n",
            "Epoch 21/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 234ms/step - accuracy: 0.8818 - loss: 0.2751 - val_accuracy: 0.8035 - val_loss: 0.4644\n",
            "Epoch 22/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 229ms/step - accuracy: 0.8957 - loss: 0.2571 - val_accuracy: 0.8120 - val_loss: 0.4685\n",
            "Epoch 23/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 240ms/step - accuracy: 0.8797 - loss: 0.2725 - val_accuracy: 0.7790 - val_loss: 0.5568\n",
            "Epoch 24/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 239ms/step - accuracy: 0.8948 - loss: 0.2518 - val_accuracy: 0.7845 - val_loss: 0.5193\n",
            "Epoch 25/25\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 235ms/step - accuracy: 0.9081 - loss: 0.2317 - val_accuracy: 0.8075 - val_loss: 0.5106\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x17cfdf99b20>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3PZasO0006Z"
      },
      "source": [
        "## Part 4 - Making a single prediction\n",
        "\n",
        "> tf.keras.utils.**load_img**(\n",
        "    <u>**path**</u>,\n",
        "    <u>**color_mode**</u>='rgb', <br>\n",
        "    <u>**target_size**</u>=None,\n",
        "    <u>**interpolation**</u>='nearest',\n",
        "    <u>**keep_aspect_ratio**</u>=False\n",
        ")\n",
        "\n",
        "> <u>**Returns:**</u> A PIL Image instance.\n",
        "\n",
        "[TensorFlow Documentation for \"load_img()\" method](https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BATCH SIZE & Expanding Image Array Dimension\n",
        "\n",
        "> The **predict()** method expects the input to be **a batch of images**, even if you are only **predicting on a single image**. The **extra dimension at the beginning of images** are generally **for batch sizes**. Batch size represent **how many images you bundle to feed the CNN at once**. The **batch can also be a single image**, however it still **needs that extra dimension of 1 to show the size of batch**. If for example batch size is 5, your input array would be of shape (5, 150, 150, 3). <br>\n",
        "The function **adds a dimension to a numpy array**. By **using \"np.expand_dims(test_image, axis=0)\"**, You load the image and get an **array of shape (150, 150, 3)**, after **calling \"expand_dims()\" in 0th axis**, you get **shape (1, 150, 150, 3)**. You are **adding an extra dimension at the beginning of the array**, effectively **creating a batch of size 1**. <br>\n",
        "This allows the model to process the single image as if it were part of a larger batch.\n",
        "\n",
        "[StackOverflow Explains Dimension Expanding of Image](https://stackoverflow.com/questions/62671394/how-to-interpret-expand-dims-while-predicting-a-uploaded-image-in-tensorflow)"
      ],
      "metadata": {
        "id": "J483yyQSqDCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsSiWEJY1BPB",
        "outputId": "c878320d-dc90-49c3-ef08-567f00662389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'cats': 0, 'dogs': 1}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_5.jpeg', target_size = (64, 64)) # cat\n",
        "test_image = image.img_to_array(test_image) # coverts the PIL image instance format into an numpy array, in order to be accepted by predict() method.\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED9KB3I54c1i",
        "outputId": "033fc964-994a-4fd3-ffd2-9c5e8a0cd08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat\n"
          ]
        }
      ],
      "source": [
        "print( 'dog' if result[0][0] == 1 else 'cat' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XGW3rJUYO-t",
        "outputId": "6b93e8ef-76a7-4adb-c36c-b875bb74a2d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "dog\n"
          ]
        }
      ],
      "source": [
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_7.jpeg', target_size = (64, 64)) # dog\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "print( 'dog' if result[0][0] == 1 else 'cat' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDq-qPRBYO-v",
        "outputId": "a928ba9c-3214-427d-84e9-a21d913ceab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "cat\n"
          ]
        }
      ],
      "source": [
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_4.jpg', target_size = (64, 64)) # cat\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "print( 'dog' if result[0][0] == 1 else 'cat' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS3nJpsXYO-v",
        "outputId": "232238c9-d96b-44b8-cdd6-90253e9bca2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "dog\n"
          ]
        }
      ],
      "source": [
        "test_image = image.load_img('dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64)) # dog\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = cnn.predict(test_image)\n",
        "print( 'dog' if result[0][0] == 1 else 'cat' )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}